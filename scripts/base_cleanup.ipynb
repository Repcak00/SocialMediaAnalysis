{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\julia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "c:\\Users\\julia\\Desktop\\sem2\\analiza_mediów\\amc_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import emoji\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download(\"words\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "from umap import UMAP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\julia\\\\Desktop\\\\sem2\\\\analiza_mediów\\\\2023-zadanie-opis-projektu-ziemniaki_rosomaki\\\\data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DIRECTORY = os.path.abspath(\"..\")\n",
    "DATA_DIR = os.path.join(DIRECTORY, \"data\")\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BonJovi.csv\n",
      "Kaleo.csv\n",
      "Kirk Fletcher.csv\n",
      "MarcusKing.csv\n",
      "Megadeth.csv\n",
      "Megadeth2.csv\n",
      "Megadeth3.csv\n",
      "Megadeth4.csv\n",
      "Megadeth5.csv\n",
      "Megadeth6.csv\n",
      "PinkFloyd.csv\n",
      "RayCharles.csv\n",
      "SystemOfADown.csv\n",
      "TheBeatles.csv\n",
      "TylerBryant&theShakedown.csv\n"
     ]
    }
   ],
   "source": [
    "combined = pd.DataFrame()\n",
    "for filename in os.listdir(DATA_DIR):\n",
    "    if filename != \".gitkeep\" and 'combined' not in filename and 'embeddings' not in filename and 'folder' not in filename:\n",
    "        print(filename)\n",
    "        f = os.path.join(DATA_DIR, filename)\n",
    "        df = pd.read_csv(f)\n",
    "        combined = pd.concat([combined, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre  channel                 \n",
       "blues  kaleo                       36050\n",
       "       kirk fletcher                8580\n",
       "       marcusking                   5591\n",
       "       raycharles                   2858\n",
       "       tylerbryant&theshakedown     2471\n",
       "metal  bonjovi                      7444\n",
       "       megadeth                    24627\n",
       "       pinkfloyd                   37842\n",
       "       systemofadown               36475\n",
       "       thebeatles                  49334\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_channel=combined.groupby(['genre','channel']).size()\n",
    "grouped_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"channel\"]=combined[\"channel\"].map(lambda x: (re.sub(r'[0-9]+', '', x)).lower()) \n",
    "combined.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 367132 entries, 0 to 372752\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   comment_id  367132 non-null  object\n",
      " 1   author      367114 non-null  object\n",
      " 2   date        367132 non-null  object\n",
      " 3   comment     367107 non-null  object\n",
      " 4   video_id    367132 non-null  object\n",
      " 5   is_reply    367132 non-null  object\n",
      " 6   parent_id   66098 non-null   object\n",
      " 7   channel     367132 non-null  object\n",
      " 8   genre       367132 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 28.0+ MB\n"
     ]
    }
   ],
   "source": [
    "combined.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_and_clear(data, comments_threshold=4):\n",
    "    \"\"\"Clears data from duplicates and authors with less than threshold comments\"\"\"\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    original_length = len(data)\n",
    "    data_grouped = (\n",
    "        data.groupby(\"author\")\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values(by=\"count\", ascending=False)\n",
    "    )\n",
    "    data_grouped = data_grouped.loc[data_grouped[\"count\"] >= comments_threshold]\n",
    "    authors = data_grouped[\"author\"].tolist()\n",
    "    data = data.loc[data[\"author\"].isin(authors)]\n",
    "    print(f\"Original length:{original_length}, after cleanup: {len(data)}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length:5220547, after cleanup: 2177322\n"
     ]
    }
   ],
   "source": [
    "# combined = group_and_clear(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_emojis(text):\n",
    "    # TODO: save emojis in a separate column\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.findall(r\"(:[^:]*:)\", text)\n",
    "    list_emoji = [emoji.emojize(x) for x in text]\n",
    "    return list_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"emoji\"] = combined[\"comment\"].map(lambda x: save_emojis(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(post):\n",
    "    post = re.sub(\"@[A-Za-z0-9]+\", \"\", post)  # Remove @ sign\n",
    "    post = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", post)\n",
    "    post = re.sub(r\"'<.*?>'\", \"\", post)  # Remove html tags\n",
    "    post = emoji.replace_emoji(post, replace=\"\")\n",
    "    post = post.replace(\"#\", \"\").replace(\n",
    "        \"_\", \" \"\n",
    "    )  # Remove hashtag sign but keep the text\n",
    "    post = \" \".join(\n",
    "        w\n",
    "        for w in nltk.wordpunct_tokenize(post)\n",
    "        if w.lower() in words or not w.isalpha()\n",
    "    )\n",
    "    regex = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
    "    post = regex.sub(\"\", post)\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"comment\"] = combined[\"comment\"].map(lambda x: cleaner(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(data, threshold=5):\n",
    "    \"\"\"Removes posts with less than threshold of words\"\"\"\n",
    "    original_length = len(data)\n",
    "    data[\"comment\"] = data[\"comment\"].map(\n",
    "        lambda x: x if len(x.split()) >= threshold else \"\"\n",
    "    )\n",
    "    data = data.loc[data[\"comment\"] != \"\"]\n",
    "    print(f\"Original length:{original_length}, after cleanup: {len(data)}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length:367132, after cleanup: 211272\n"
     ]
    }
   ],
   "source": [
    "combined = sentence_length(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(os.path.join(DATA_DIR, \"combined.csv\"), index=False)\n",
    "combined.to_feather(os.path.join(DATA_DIR, \"combined.feather\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"papluca/xlm-roberta-base-language-detection\", max_length=512\n",
    ")\n",
    "model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"papluca/xlm-roberta-base-language-detection\", max_length=512\n",
    ")\n",
    "\n",
    "\n",
    "def lang_detector(text):\n",
    "    \"\"\"Detects language of the post\"\"\"\n",
    "    # TO DO : FIX ERROR WITH TOKENIZER LENGTH (too long sequences)\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    )\n",
    "    inputs.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    lang = model.config.id2label[predicted_class_id]\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"language\"] = combined[\"comment\"].map(lambda x: lang_detector(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_lang(data):\n",
    "    \"\"\"Deletes posts that are not in English\"\"\"\n",
    "    original_length = len(data)\n",
    "    data = data.loc[data[\"language\"] == \"en\"]\n",
    "    data.drop(columns=[\"language\"], inplace=True)\n",
    "    print(f\"Original length:{original_length}, after cleanup: {len(data)}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length:211272, after cleanup: 191347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_12780\\1205206985.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop(columns=[\"language\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "combined = clear_lang(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "metal    139584\n",
       "blues     51763\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_genre=combined.groupby(['genre']).size().sort_values(ascending=False)\n",
    "grouped_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre  channel                 \n",
       "blues  kaleo                       32700\n",
       "       kirk fletcher                8480\n",
       "       marcusking                   5497\n",
       "       raycharles                   2657\n",
       "       tylerbryant&theshakedown     2429\n",
       "metal  bonjovi                      6164\n",
       "       megadeth                    22720\n",
       "       pinkfloyd                   35602\n",
       "       systemofadown               31358\n",
       "       thebeatles                  43740\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# le = LabelEncoder()\n",
    "# combined[\"author_label\"] = le.fit_transform(combined[\"channel\"])\n",
    "# combined[\"genre_label\"] = le.fit_transform(combined[\"genre\"])\n",
    "grouped_channel=combined.groupby(['genre','channel']).size()\n",
    "grouped_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(os.path.join(DATA_DIR, \"combined_final.csv\"), index=False, encoding='utf-8')\n",
    "# combined.to_feather(os.path.join(DATA_DIR, \"combined_final.feather\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "sentences = combined[\"comment\"].values.tolist()\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR,'embeddings_final.npy'), 'wb') as f:\n",
    "    np.save(f, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2320602\n",
      "2300677\n"
     ]
    }
   ],
   "source": [
    "grouped_channel=combined.groupby(['genre','channel']).size()\n",
    "grouped_channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(DATA_DIR,'embeddings_final.npy'), 'rb') as f:\n",
    "#     embeddings = np.load(f)\n",
    "# combined=pd.read_csv(os.path.join(DATA_DIR, \"combined_final.csv\"), encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
